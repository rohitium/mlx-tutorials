{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Inference\n",
    "\n",
    "[Link to MLX Tutorial](https://ml-explore.github.io/mlx/build/html/examples/llama-inference.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import math\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    def __init__(self, dims: int, num_heads: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.rope = nn.RoPE(dims // num_heads, traditional=True)\n",
    "        self.query_proj = nn.Linear(dims, dims, bias=False)\n",
    "        self.key_proj = nn.Linear(dims, dims, bias=False)\n",
    "        self.value_proj = nn.Linear(dims, dims, bias=False)\n",
    "        self.out_proj = nn.Linear(dims, dims, bias=False)\n",
    "\n",
    "    def __call__(self, queries, keys, values, mask=None, cache=None):\n",
    "        queries = self.query_proj(queries)\n",
    "        keys = self.key_proj(keys)\n",
    "        values = self.value_proj(values)\n",
    "\n",
    "        # Extract some shapes\n",
    "        num_heads = self.num_heads\n",
    "        B, L, D = queries.shape\n",
    "\n",
    "        # Prepare the queries, keys and values for the attention computation\n",
    "        queries = queries.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3)\n",
    "        keys = keys.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3)\n",
    "        values = values.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3)\n",
    "\n",
    "        # Add RoPE to the queries and keys and combine them with the cache\n",
    "        if cache is not None:\n",
    "            key_cache, value_cache = cache\n",
    "            queries = self.rope(queries, offset=key_cache.shape[2])\n",
    "            keys = self.rope(keys, offset=key_cache.shape[2])\n",
    "            keys = mx.concatenate([key_cache, keys], axis=2)\n",
    "            values = mx.concatenate([value_cache, values], axis=2)\n",
    "        else:\n",
    "            queries = self.rope(queries)\n",
    "            keys = self.rope(keys)\n",
    "\n",
    "        # Finally perform the attention computation\n",
    "        scale = math.sqrt(1 / queries.shape[-1])\n",
    "        scores = (queries * scale) @ keys.transpose(0, 1, 3, 2)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask\n",
    "        scores = mx.softmax(scores, axis=-1)\n",
    "        values_hat = (scores @ values).transpose(0, 2, 1, 3).reshape(B, L, -1)\n",
    "\n",
    "        # Note that we return the keys and values to possibly be used as a cache\n",
    "        return self.out_proj(values_hat), (keys, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaEncoderLayer(nn.Module):\n",
    "    def __init__(self, dims: int, mlp_dims: int, num_heads: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = LlamaAttention(dims, num_heads)\n",
    "\n",
    "        self.norm1 = nn.RMSNorm(dims)\n",
    "        self.norm2 = nn.RMSNorm(dims)\n",
    "\n",
    "        self.linear1 = nn.Linear(dims, mlp_dims, bias=False)\n",
    "        self.linear2 = nn.Linear(dims, mlp_dims, bias=False)\n",
    "        self.linear3 = nn.Linear(mlp_dims, dims, bias=False)\n",
    "\n",
    "    def __call__(self, x, mask=None, cache=None):\n",
    "        y = self.norm1(x)\n",
    "        y, cache = self.attention(y, y, y, mask, cache)\n",
    "        x = x + y\n",
    "\n",
    "        y = self.norm2(x)\n",
    "        a = self.linear1(y)\n",
    "        b = self.linear2(y)\n",
    "        y = a * mx.sigmoid(a) * b\n",
    "        y = self.linear3(y)\n",
    "        x = x + y\n",
    "\n",
    "        return x, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_layers: int, vocab_size: int, dims: int, mlp_dims: int, num_heads: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, dims)\n",
    "        self.layers = [\n",
    "            LlamaEncoderLayer(dims, mlp_dims, num_heads) for _ in range(num_layers)\n",
    "        ]\n",
    "        self.norm = nn.RMSNorm(dims)\n",
    "        self.out_proj = nn.Linear(dims, vocab_size, bias=False)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        mask = nn.MultiHeadAttention.create_additive_causal_mask(x.shape[1])\n",
    "        mask = mask.astype(self.embedding.weight.dtype)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        for l in self.layers:\n",
    "            x, _ = l(x, mask)\n",
    "        x = self.norm(x)\n",
    "        return self.out_proj(x)\n",
    "\n",
    "    def generate(self, x, temp=1.0):\n",
    "        cache = []\n",
    "\n",
    "        # Make an additive causal mask. We will need that to process the prompt.\n",
    "        mask = nn.MultiHeadAttention.create_additive_causal_mask(x.shape[1])\n",
    "        mask = mask.astype(self.embedding.weight.dtype)\n",
    "\n",
    "        # First we process the prompt x the same way as in __call__ but\n",
    "        # save the caches in cache\n",
    "        x = self.embedding(x)\n",
    "        for l in self.layers:\n",
    "            x, c = l(x, mask=mask)\n",
    "            cache.append(c)  # <--- we store the per layer cache in a\n",
    "                             #      simple python list\n",
    "        x = self.norm(x)\n",
    "        y = self.out_proj(x[:, -1])  # <--- we only care about the last logits\n",
    "                                     #      that generate the next token\n",
    "        y = mx.random.categorical(y * (1/temp))\n",
    "\n",
    "        # y now has size [1]\n",
    "        # Since MLX is lazily evaluated nothing is computed yet.\n",
    "        # Calling y.item() would force the computation to happen at\n",
    "        # this point but we can also choose not to do that and let the\n",
    "        # user choose when to start the computation.\n",
    "        yield y\n",
    "\n",
    "        # Now we parsed the prompt and generated the first token we\n",
    "        # need to feed it back into the model and loop to generate the\n",
    "        # rest.\n",
    "        while True:\n",
    "            # Unsqueezing the last dimension to add a sequence length\n",
    "            # dimension of 1\n",
    "            x = y[:, None]\n",
    "\n",
    "            x = self.embedding(x)\n",
    "            for i in range(len(cache)):\n",
    "                # We are overwriting the arrays in the cache list. When\n",
    "                # the computation will happen, MLX will be discarding the\n",
    "                # old cache the moment it is not needed anymore.\n",
    "                x, cache[i] = self.layers[i](x, mask=None, cache=cache[i])\n",
    "            x = self.norm(x)\n",
    "            y = self.out_proj(x[:, -1])\n",
    "            y = mx.random.categorical(y * (1/temp))\n",
    "\n",
    "            yield y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Llama(num_layers=12, vocab_size=8192, dims=512, mlp_dims=1024, num_heads=8)\n",
    "\n",
    "# Since MLX is lazily evaluated nothing has actually been materialized yet.\n",
    "# We could have set the `dims` to 20_000 on a machine with 8GB of RAM and the\n",
    "# code above would still run. Let's actually materialize the model.\n",
    "mx.eval(model.parameters())\n",
    "\n",
    "prompt = mx.array([[1, 10, 8, 32, 44, 7]])  # <-- Note the double brackets because we\n",
    "                                            #     have a batch dimension even\n",
    "                                            #     though it is 1 in this case\n",
    "\n",
    "generated = [t for i, t in zip(range(10), model.generate(prompt, 0.8))]\n",
    "\n",
    "# Since we haven't evaluated anything, nothing is computed yet. The list\n",
    "# `generated` contains the arrays that hold the computation graph for the\n",
    "# full processing of the prompt and the generation of 10 tokens.\n",
    "#\n",
    "# We can evaluate them one at a time, or all together. Concatenate them or\n",
    "# print them. They would all result in very similar runtimes and give exactly\n",
    "# the same results.\n",
    "mx.eval(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx_tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
